#!/usr/bin/env python3
"""
MOSDAC AI Help Bot - Master Control File
========================================

This is the master file that orchestrates all existing components:
1. Uses existing crawl4ai_mosdac.py for scraping
2. Uses existing ingest.py for data ingestion
3. Uses existing chat.py for AI chat
4. Provides unified control interface

Author: AI Assistant
Date: 2025-01-10
Version: 1.0
"""

import asyncio
import json
import os
import subprocess
import sys
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MOSDACBot:
    """Master control for MOSDAC AI Help Bot using existing components"""
    
    def __init__(self):
        # Configuration
        self.crawl_output_dir = Path("./mosdac_complete_data")
        self.chroma_dir = Path("./chroma_db")
        self.llm_available = False
        
        # Check if components are available
        self.check_components()
    
    def check_components(self):
        """Check if all required components are available"""
        components = {
            "crawler": Path("crawl4ai_mosdac.py").exists(),
            "ingest": Path("ingest.py").exists(),
            "chat": Path("chat.py").exists(),
            "config": Path("config.py").exists(),
            "llm_loader": Path("models/llm_loader.py").exists()
        }
        
        missing = [name for name, exists in components.items() if not exists]
        if missing:
            logger.warning(f"Missing components: {missing}")
        
        # Check if LLM is available
        try:
            from models.llm_loader import get_llm_info
            info = get_llm_info()
            self.llm_available = info.get("available", False)
            if self.llm_available:
                mode = info.get("mode", "unknown")
                logger.info(f"âœ… LLM is available (mode: {mode})")
            else:
                mode = info.get("mode", "unknown")
                logger.warning(f"âŒ LLM not available (mode: {mode})")
        except Exception as e:
            self.llm_available = False
            logger.warning(f"âŒ LLM not available: {e}")
    
    async def scrape_data(self):
        """Run the comprehensive scraper to scrape ALL MOSDAC data"""
        logger.info("ğŸ” Starting comprehensive data scraping...")
        
        try:
            # Import and run the comprehensive scraper
            from comprehensive_mosdac_scraper import ComprehensiveMOSDACScraper
            
            # Use comprehensive scraper
            scraper = ComprehensiveMOSDACScraper()
            stats = await scraper.run_comprehensive_scraping()
            
            if stats["urls_processed"] > 0:
                logger.info("âœ… Comprehensive scraping completed successfully!")
                logger.info(f"ğŸ“Š Processed: {stats['urls_processed']} URLs")
                logger.info(f"ğŸ“„ Total content: {stats['total_content_length']:,} characters")
                logger.info(f"ğŸ“Š Tables extracted: {stats['tables_extracted']}")
                return True
            else:
                logger.error("âŒ No URLs were processed successfully!")
                return False
            
        except Exception as e:
            logger.error(f"âŒ Error during comprehensive scraping: {e}")
            return False
    
    def ingest_data(self):
        """Run the existing ingestion pipeline with comprehensive data"""
        logger.info("ğŸ“¥ Starting data ingestion...")
        
        try:
            # Import and run the existing ingestion
            from ingest import ModernIngestionPipeline
            
            # Use the comprehensive data directory
            path = str(self.crawl_output_dir)
            
            logger.info(f"Ingesting from: {path}")
            
            # Check if comprehensive data exists
            if not self.crawl_output_dir.exists():
                logger.error(f"âŒ Comprehensive data directory not found: {path}")
                logger.info("ğŸ’¡ Run comprehensive scraping first (option 1)")
                return False
            
            # Create and run ingestion pipeline
            pipeline = ModernIngestionPipeline()
            result = pipeline.run_ingestion(path)
            
            if result.get("success", False):
                logger.info("âœ… Data ingestion completed successfully!")
                logger.info(f"ğŸ“Š Documents: {result.get('documents_loaded', 0)}")
                logger.info(f"ğŸ“„ Chunks: {result.get('chunks_stored', 0)}")
                logger.info(f"â±ï¸ Time: {result.get('processing_time', 0):.2f}s")
                return True
            else:
                logger.error(f"âŒ Data ingestion failed: {result.get('error', 'Unknown error')}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Error during ingestion: {e}")
            return False
    
    def start_chat(self):
        """Start the existing chat system"""
        logger.info("ğŸ’¬ Starting chat system...")
        
        if not self.llm_available:
            print("âŒ LLM not available. Please check your configuration:")
            print("   â€¢ For API mode: Set GEMINI_API_KEY environment variable")
            print("   â€¢ For Ollama mode: Set LLM_MODE=ollama and ensure Ollama is running")
            return
        
        try:
            # Import and start the existing chat system
            from chat import start_modern_chat
            start_modern_chat()
            
        except Exception as e:
            logger.error(f"âŒ Error starting chat: {e}")
            print(f"Error: {e}")
    
    def get_data_status(self) -> Dict[str, Any]:
        """Get current data status"""
        status = {
            "scraped_data": {
                "pages_count": 0,
                "total_content_length": 0,
                "last_scraped": None
            },
            "vector_database": {
                "collection_exists": False,
                "document_count": 0
            },
            "components": {
                "crawler_available": Path("crawl4ai_mosdac.py").exists(),
                "ingest_available": Path("ingest.py").exists(),
                "chat_available": Path("chat.py").exists(),
                "llm_available": self.llm_available
            }
        }
        
        # Check scraped data
        if self.crawl_output_dir.exists():
            # Try comprehensive index first, then fallback to crawling summary
            summary_file = self.crawl_output_dir / "comprehensive_index.json"
            if not summary_file.exists():
                summary_file = self.crawl_output_dir / "crawling_summary.json"
            
            if summary_file.exists():
                try:
                    with open(summary_file, 'r') as f:
                        summary = json.load(f)
                        # Handle both comprehensive_index.json and crawling_summary.json formats
                        if "statistics" in summary:
                            # Comprehensive index format
                            stats = summary.get("statistics", {})
                            status["scraped_data"]["pages_count"] = stats.get("urls_processed", 0)
                            status["scraped_data"]["last_scraped"] = summary.get("metadata", {}).get("scraping_session", {}).get("completed_at")
                            status["scraped_data"]["total_content_length"] = stats.get("total_content_length", 0)
                        else:
                            # Crawling summary format
                            status["scraped_data"]["pages_count"] = summary.get("total_pages", 0)
                            status["scraped_data"]["last_scraped"] = summary.get("crawling_completed_at") or summary.get("timestamp")
                        
                        # Calculate total content length
                        total_length = 0
                        for page_dir in self.crawl_output_dir.iterdir():
                            if page_dir.is_dir():
                                content_file = page_dir / "content.md"
                                if content_file.exists():
                                    total_length += len(content_file.read_text())
                        status["scraped_data"]["total_content_length"] = total_length
                except Exception as e:
                    logger.warning(f"Failed to read summary file: {e}")
            else:
                # Count pages manually if no summary file
                page_count = 0
                total_length = 0
                for page_dir in self.crawl_output_dir.iterdir():
                    if page_dir.is_dir():
                        page_count += 1
                        content_file = page_dir / "content.md"
                        if content_file.exists():
                            total_length += len(content_file.read_text())
                status["scraped_data"]["pages_count"] = page_count
                status["scraped_data"]["total_content_length"] = total_length
        
        # Check vector database
        if self.chroma_dir.exists():
            try:
                import chromadb
                client = chromadb.PersistentClient(path=str(self.chroma_dir))
                collections = client.list_collections()
                if collections:
                    status["vector_database"]["collection_exists"] = True
                    # Try to get document count
                    try:
                        collection = client.get_collection("mosdac_collection")
                        count = collection.count()
                        status["vector_database"]["document_count"] = count
                    except:
                        pass
            except Exception as e:
                logger.warning(f"Failed to check vector database: {e}")
        
        return status
    
    def _get_llm_info(self) -> Dict[str, Any]:
        """Get LLM configuration info"""
        try:
            from models.llm_loader import get_llm_info
            return get_llm_info()
        except Exception as e:
            return {
                "mode": "unknown",
                "available": False,
                "error": str(e)
            }
    
    def remove_data(self):
        """Remove all scraped data and vector database"""
        logger.info("ğŸ—‘ï¸ Removing all data...")
        
        # Remove scraped data
        if self.crawl_output_dir.exists():
            import shutil
            shutil.rmtree(self.crawl_output_dir)
            logger.info("âœ… Removed scraped data")
        
        # Remove vector database
        if self.chroma_dir.exists():
            import shutil
            shutil.rmtree(self.chroma_dir)
            logger.info("âœ… Removed vector database")
        
        # Recreate crawl output directory
        self.crawl_output_dir.mkdir(exist_ok=True)
        
        logger.info("âœ… Data removal completed")
    
    def scrape_and_ingest(self):
        """Complete workflow: scrape and ingest data"""
        logger.info("ğŸš€ Starting complete workflow: scrape + ingest")
        
        # Step 1: Scrape data
        scrape_success = asyncio.run(self.scrape_data())
        if not scrape_success:
            logger.error("âŒ Scraping failed, aborting workflow")
            return False
        
        # Step 2: Ingest data
        ingest_success = self.ingest_data()
        if not ingest_success:
            logger.error("âŒ Ingestion failed")
            return False
        
        logger.info("âœ… Complete workflow finished successfully!")
        return True

def show_menu():
    """Show main menu"""
    print("\n" + "="*60)
    print("ğŸ›°ï¸  MOSDAC AI Help Bot - Master Control")
    print("="*60)
    print("[1] ğŸ” Scrape Data Only")
    print("[2] ğŸ“¥ Ingest Data Only")
    print("[3] ğŸš€ Scrape + Ingest (Complete Workflow)")
    print("[4] ğŸ’¬ Chat with Bot")
    print("[5] ğŸ“Š Check Data Status")
    print("[6] ğŸ—‘ï¸  Remove All Data")
    print("[7] ğŸ”„ Re-scrape + Re-ingest")
    print("[8] âŒ Exit")
    print("="*60)

async def main():
    """Main function"""
    bot = MOSDACBot()
    
    while True:
        show_menu()
        choice = input("Select an option (1-8): ").strip()
        
        if choice == "1":
            print("\nğŸ” Starting data scraping...")
            success = await bot.scrape_data()
            if success:
                print("âœ… Data scraping completed successfully!")
            else:
                print("âŒ Data scraping failed!")
        
        elif choice == "2":
            print("\nğŸ“¥ Starting data ingestion...")
            success = bot.ingest_data()
            if success:
                print("âœ… Data ingestion completed successfully!")
            else:
                print("âŒ Data ingestion failed!")
        
        elif choice == "3":
            print("\nğŸš€ Starting complete workflow...")
            success = bot.scrape_and_ingest()
            if success:
                print("âœ… Complete workflow finished successfully!")
            else:
                print("âŒ Workflow failed!")
        
        elif choice == "4":
            print("\nğŸ’¬ Starting chat mode...")
            bot.start_chat()
        
        elif choice == "5":
            print("\nğŸ“Š Current Data Status:")
            status = bot.get_data_status()
            llm_info = bot._get_llm_info()
            
            print(f"ğŸ“„ Scraped Data:")
            print(f"   Pages: {status['scraped_data']['pages_count']}")
            print(f"   Total Content: {status['scraped_data']['total_content_length']:,} characters")
            print(f"   Last Scraped: {status['scraped_data']['last_scraped'] or 'Never'}")
            
            print(f"\nğŸ—„ï¸  Vector Database:")
            print(f"   Collection Exists: {status['vector_database']['collection_exists']}")
            print(f"   Documents: {status['vector_database']['document_count']}")
            
            print(f"\nğŸ¤– LLM Configuration:")
            print(f"   Mode: {llm_info.get('mode', 'unknown')}")
            if llm_info.get('mode') == 'api':
                print(f"   API Key: {'âœ… Set' if llm_info.get('api_key_set') else 'âŒ Missing'}")
            elif llm_info.get('mode') == 'ollama':
                print(f"   Model: {llm_info.get('ollama_model', 'unknown')}")
                print(f"   URL: {llm_info.get('ollama_url', 'unknown')}")
            print(f"   Available: {'âœ…' if llm_info.get('available') else 'âŒ'}")
            
            print(f"\nğŸ”§ Components:")
            print(f"   Crawler: {'âœ…' if status['components']['crawler_available'] else 'âŒ'}")
            print(f"   Ingest: {'âœ…' if status['components']['ingest_available'] else 'âŒ'}")
            print(f"   Chat: {'âœ…' if status['components']['chat_available'] else 'âŒ'}")
        
        elif choice == "6":
            confirm = input("âš ï¸  Are you sure you want to remove ALL data? (yes/no): ").strip().lower()
            if confirm == "yes":
                bot.remove_data()
                print("âœ… All data removed successfully!")
            else:
                print("âŒ Data removal cancelled.")
        
        elif choice == "7":
            print("\nğŸ”„ Re-scraping and re-ingesting data...")
            bot.remove_data()
            success = bot.scrape_and_ingest()
            if success:
                print("âœ… Re-scraping and re-ingestion completed successfully!")
            else:
                print("âŒ Re-scraping and re-ingestion failed!")
        
        elif choice == "8":
            print("ğŸ‘‹ Goodbye!")
            break
        
        else:
            print("âŒ Invalid choice. Please try again.")

if __name__ == "__main__":
    asyncio.run(main())