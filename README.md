
# PRIVATA

Privata is a lightweight, offline-first document ingestion and querying tool that uses local embeddings and cloud-based LLMs (Google Gemini) to answer questions from your private files â€” with no server, no browser, and no GPU required.

It follows the RAG (Retrieval-Augmented Generation) architecture and runs entirely via CLI. Users can ingest PDFs, markdown, and text files, then interact with them using AI-powered semantic search.

---

## âœ… Core Features

- ğŸ“ Ingest local `.pdf`, `.txt`, and `.md` files
- ğŸ§© Auto-chunks and embeds content using HuggingFace models
- ğŸ” Stores vectors in lightweight `ChromaDB`
- ğŸ”„ Retrieves relevant context using vector similarity
- ğŸ¤– Sends context + query to Google Gemini (`gemini-1.5-flash`)
- ğŸ§  Returns accurate, grounded answers â€” even for technical PDFs
- ğŸ–¥ï¸ Fully CLI-based (no browser or web server needed)
- âš™ï¸ Configurable via single `config.py` file

---

## ğŸ§  Architecture (High-Level)

```plaintext
           +------------+         +-------------------+
           |   PDFs     |         |   .txt / .md      |
           +------------+         +-------------------+
                   â”‚
                   â–¼
          [ Unstructured Loaders ]
                   â”‚
                   â–¼
         [ LangChain Text Splitter ]
                   â”‚
                   â–¼
         [ Sentence Transformers ]
                   â”‚
                   â–¼
             [ ChromaDB Store ]
                   â”‚
                   â–¼
   +--------- User Query (CLI Input) --------+
   |                                         |
   â–¼                                         â–¼
[ Embed Query ]                     [ Retrieve Top-K Chunks ]
            \______________________/ 
                       â”‚
                       â–¼
        [ Gemini Prompt Construction ]
                       â”‚
                       â–¼
        [ Google Gemini API (gemini-1.5-flash) ]
                       â”‚
                       â–¼
              [ Final Response Output ]
```

---

## âœ¨ Key Features

- **Offline Ingestion** â€“ Parse and embed your local `.pdf`, `.txt`, and `.md` files entirely offline.
- **RAG Pipeline** â€“ Combines vector similarity search with LLM generation for grounded, context-aware answers.
- **Local Vector DB** â€“ Uses `ChromaDB` for lightweight, persistent local document storage.
- **Cloud LLM (Gemini)** â€“ Uses `gemini-1.5-flash` via Google's `google-generativeai` SDK for high-speed, accurate generation.
- **Modular Architecture** â€“ Cleanly separated modules for ingestion, embeddings, vector DB, and LLM API.
- **CLI Interface** â€“ Run everything from a terminal â€” no server, browser, or UI required.
- **Configurable** â€“ Customize chunk sizes, models, prompts, and retrieval depth via `config.py`.

---

## ğŸ§­ Use Case Example

You can load 100s of pages of PDF textbooks or technical docs, then ask:

- _"What is Unit 4 in Java?"_
- _"What is the difference between stacks and queues?"_
- _"Summarize the networking protocols chapter."_

Gemini will ground its answer in your actual documents, not public internet.

---

## ğŸ—‚ï¸ Project Structure

```plaintext
privata/
â”œâ”€â”€ main.py                # CLI menu entry point
â”œâ”€â”€ ingest.py              # Document ingestion + chunking pipeline
â”œâ”€â”€ chat.py                # Chat loop: query â†’ retrieve â†’ respond
â”œâ”€â”€ config.py              # Central config for models, chunking, prompts
â”œâ”€â”€ requirements.txt       # Dependency list
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ llm_loader.py      # Gemini API integration
â”‚
â”œâ”€â”€ retriever/
â”‚   â”œâ”€â”€ embedder.py        # HuggingFace embedding logic (MiniLM)
â”‚   â””â”€â”€ vectordb.py        # ChromaDB interface for storing & retrieving
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ doc_loader.py      # Loads & parses PDF, text, markdown files
â”‚
â”œâ”€â”€ db/                    # (Auto-generated) stores vector data
â”‚
â”œâ”€â”€ .env.example           # Sample structure for GEMINI_API_KEY
â”œâ”€â”€ .gitignore             # Ignores pycache, env, vector DB, temp files
â””â”€â”€ README.md              # This documentation file
```

---

## âš™ï¸ Setup Instructions

### 1. Clone the repository and install dependencies

```bash
git clone  https://github.com/Aayushbankar/privata.git
cd privata
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2. Configure your Gemini API Key

Obtain your API key from https://ai.google.dev.

Then either:

**Option A: Create a `.env` file**

```bash
cp .env.example .env
```

Inside `.env`, add:

```env
GEMINI_API_KEY=your_actual_key_here
```

**Option B: Export key manually in terminal**

```bash
export GEMINI_API_KEY=your_actual_key_here
```

---

## ğŸ“¥ Ingesting Documents

```bash
python main.py
```

Then:

1. Select `[1] Ingest Documents]`
2. Enter path to a folder containing `.pdf`, `.txt`, or `.md` files
3. Wait while the system loads, chunks, embeds, and stores documents locally into `./db/`

---

## ğŸ’¬ Starting the Chat

```bash
python main.py
```

Then:

1. Select `[2] Start Chatbot`
2. Ask questions related to the documents you ingested

Example prompts:

- "What is Unit 2 in Java?"
- "List all topics covered in Python basics."
- "Summarize the syllabus for Data Structures."

---

## ğŸ”§ Configuration

All settings are in `config.py`:

```python
chat = {
    "llm_model": "gemini-1.5-flash",
    "top_k": 5,
    "streaming": False,
    "temperature": 0.3,
    "prompt_template": """You are a helpful assistant. Use the following context to answer:

{context}

Question: {query}
Answer:"""
}

ingest = {
    "chunk_size": 500,
    "chunk_overlap": 50
}
```

---

## ğŸ§¯ Troubleshooting

- **Gemini API Error 404**  
  â†’ You're using the wrong model name or version. Use `"models/gemini-1.5-flash"`

- **No answer or poor results**  
  â†’ Check if you ingested the correct folder. Also try raising `top_k` in config.

- **High memory usage during ingestion**  
  â†’ PDFs with scanned pages or massive length can cause slow embedding. Try ingesting a smaller folder first.

- **FontBBox warnings**  
  â†’ These are non-fatal parsing issues from `pdfminer`. Safe to ignore.

---

## ğŸš€ Future Improvements

- Add web-based UI (FastAPI or Streamlit)
- Use async streaming Gemini responses
- Replace Chroma with Faiss or Weaviate for scale
- GPU-backed embedding for large corpus
- Docker containerization and hosted inference

---

## ğŸ“„ License

MIT License. See `LICENSE` file for details.

---

Built with â¤ï¸ for working locally, thinking deeply, and querying privately.
