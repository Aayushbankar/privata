# MOSDAC AI Help Bot

An AI-powered help bot for information retrieval from the MOSDAC (Meteorological and Oceanographic Satellite Data Archival Center) website. Features both CLI interface and REST API with automatic web scraping capabilities.

## ğŸš€ Quick Start

```bash
# Run the bot (CLI mode)
./scripts/run_bot.sh

# Or manually
python main.py

# Start the REST API server
python -m src.api.main

# Access comprehensive HTML documentation
open index.html
```

## ğŸ“ Project Structure

```
privata/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ api/                      # REST API implementation
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â”‚   â”œâ”€â”€ config.py            # API configuration
â”‚   â”‚   â”œâ”€â”€ dependencies.py      # API dependencies
â”‚   â”‚   â”œâ”€â”€ models/              # Pydantic models
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py          # Chat models
â”‚   â”‚   â”‚   â”œâ”€â”€ status.py        # Status models
â”‚   â”‚   â”‚   â”œâ”€â”€ data.py          # Data job models
â”‚   â”‚   â”‚   â””â”€â”€ admin.py         # Admin models
â”‚   â”‚   â”œâ”€â”€ routes/              # API endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py          # Chat endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ status.py        # Status endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ data.py          # Data endpoints
â”‚   â”‚   â”‚   â””â”€â”€ admin.py         # Admin endpoints
â”‚   â”‚   â””â”€â”€ background/          # Background tasks
â”‚   â”‚       â””â”€â”€ scheduler.py     # Auto-scraping scheduler
â”‚   â”œâ”€â”€ core/                     # Core functionality
â”‚   â”‚   â”œâ”€â”€ mosdac_bot.py        # Main bot controller
â”‚   â”‚   â”œâ”€â”€ config.py            # Configuration
â”‚   â”‚   â””â”€â”€ config.json          # Config file
â”‚   â”œâ”€â”€ scrapers/                 # Web scraping
â”‚   â”‚   â”œâ”€â”€ comprehensive_mosdac_scraper.py
â”‚   â”‚   â””â”€â”€ crawl4ai_mosdac.py
â”‚   â”œâ”€â”€ ingestion/                # Data ingestion
â”‚   â”‚   â””â”€â”€ ingest.py
â”‚   â”œâ”€â”€ chat/                     # Chat system
â”‚   â”‚   â””â”€â”€ chat.py
â”‚   â”œâ”€â”€ models/                   # LLM integration
â”‚   â”‚   â””â”€â”€ llm_loader.py
â”‚   â”œâ”€â”€ retrieval/                # Vector search & retrieval
â”‚   â”‚   â”œâ”€â”€ modern_vectordb.py
â”‚   â”‚   â”œâ”€â”€ multi_modal_embedder.py
â”‚   â”‚   â””â”€â”€ reranker.py
â”‚   â””â”€â”€ utils/                    # Utilities
â”‚       â”œâ”€â”€ enhanced_chunker.py
â”‚       â”œâ”€â”€ enhanced_doc_loader.py
â”‚       â””â”€â”€ structured_extractor.py
â”œâ”€â”€ data/                         # Data storage
â”‚   â”œâ”€â”€ scraped/                  # Scraped website data
â”‚   â”‚   â””â”€â”€ mosdac_complete_data/
â”‚   â””â”€â”€ vector_db/                # Vector database
â”‚       â””â”€â”€ chroma_db/
â”œâ”€â”€ config/                       # System configuration
â”‚   â””â”€â”€ system_config.json        # API configuration file
â”œâ”€â”€ scripts/                      # Utility scripts
â”‚   â”œâ”€â”€ run_bot.sh               # Bot runner
â”‚   â”œâ”€â”€ setup_llm.py             # LLM setup helper
â”‚   â”œâ”€â”€ main.py                  # Legacy main
â”‚   â””â”€â”€ advanced_rag_ingestion.py
â”œâ”€â”€ tests/                        # Test files
â”‚   â”œâ”€â”€ test.py
â”‚   â””â”€â”€ test_chat.py
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ PROJECT_PROGRESS_REPORT.md
â”‚   â”œâ”€â”€ PROJECT_STATUS_REPORT.md
â”‚   â”œâ”€â”€ API_DEVELOPMENT_JOURNAL.md
â”‚   â”œâ”€â”€ API_FILE_DOCUMENTATION.md
â”‚   â””â”€â”€ MASTER_DEVELOPMENT_JOURNAL.md
â”œâ”€â”€ main.py                       # Main entry point
â”œâ”€â”€ requirements.txt              # Dependencies
â”œâ”€â”€ API_DOCUMENTATION.md          # API documentation
â”œâ”€â”€ API_README.md                 # API quick start guide
â”œâ”€â”€ index.html                    # Comprehensive HTML documentation
â””â”€â”€ README.md                     # This file
```

## ğŸ› ï¸ Features

### Core Bot Features
- **Comprehensive Web Scraping**: 443 URLs processed (7x more than required)
- **RAG-Optimized Data Extraction**: 4.1M+ characters, 270 structured tables
- **Advanced Ingestion Pipeline**: 708 semantic chunks stored in ChromaDB
- **Fully Functional Chat System**: Natural language Q&A with citations
- **Dual LLM Support**: Gemini API + Ollama offline modes
- **Complete Data Management**: Status monitoring, removal, re-scraping

### REST API Features
- **Production-Ready API**: FastAPI-based RESTful endpoints
- **Auto-Scraping System**: Automatic website scraping every 48 hours
- **Background Job Processing**: Asynchronous scraping and ingestion jobs
- **Real-time Chat API**: AI-powered chat with MOSDAC content
- **Configuration Management**: Dynamic system configuration via API
- **Health Monitoring**: Comprehensive system status and metrics
- **Rate Limiting**: Built-in protection against abuse

## ğŸ”§ Setup

1. **Install Dependencies**:
```bash
pip install -r requirements.txt
```

2. **Configure LLM**:
```bash
# For API mode (recommended)
export GEMINI_API_KEY="your-api-key"
export LLM_MODE="api"

# For Ollama mode (offline)
export LLM_MODE="ollama"
```

3. **Run Setup Helper**:
```bash
python scripts/setup_llm.py
```

4. **Start API Server**:
```bash
python -m src.api.main
```

## ğŸ¯ Usage

### CLI Interface
```bash
# Main Bot Interface
python main.py

# Direct Core Access
python src/core/mosdac_bot.py

# Test System
python tests/test_chat.py
```

### REST API Interface
```bash
# Start API server
python -m src.api.main

# Test API endpoints
python test_api.py

# Access API documentation
# Swagger UI: http://localhost:8000/api/docs
# ReDoc: http://localhost:8000/api/redoc
```

### Auto-Scraping Configuration
The API includes automatic scraping every 48 hours. Configure via:
```bash
curl -X PUT "http://localhost:8000/api/v1/admin/config" \
  -H "Content-Type: application/json" \
  -d '{"scraping_interval_hours": 24}'
```

## ğŸ“Š Current Status

- **Pages Scraped**: 443 URLs
- **Content Volume**: 4.1M+ characters
- **Vector Database**: 708 chunks indexed
- **Tables Extracted**: 270 structured tables
- **Quality Score**: 0.63 average
- **API Status**: Production-ready with auto-scraping

## ğŸ“‹ API Endpoints

### Chat
- `POST /api/v1/chat` - Send message to AI bot
- `GET /api/v1/chat/sessions` - Get active chat sessions

### Data Management
- `POST /api/v1/data/scrape` - Initiate scraping job
- `GET /api/v1/data/scrape/{job_id}` - Get scraping job status
- `POST /api/v1/data/ingest` - Initiate ingestion job
- `GET /api/v1/data/ingest/{job_id}` - Get ingestion job status

### System Status
- `GET /api/v1/status` - Comprehensive system status
- `GET /health` - Quick health check

### Administration
- `GET /api/v1/admin/config` - Get system configuration
- `PUT /api/v1/admin/config` - Update system configuration
- `GET /api/v1/admin/jobs` - List all jobs
- `POST /api/v1/admin/jobs/cancel/{job_id}` - Cancel a job

## ğŸ“ Documentation

### Comprehensive HTML Documentation
Access the full documentation website by opening `index.html` in your browser. Features include:

- **Complete Usage Guide**: Step-by-step instructions for all CLI options
- **API Documentation**: Detailed endpoint descriptions with examples
- **Client Examples**: Python and JavaScript code samples
- **Troubleshooting**: Solutions for common issues
- **Performance Metrics**: System requirements and benchmarks
- **Configuration Guide**: Auto-scraping schedule and system settings

### Additional Documentation Files
- `API_DOCUMENTATION.md` - Complete API reference
- `API_README.md` - API quick start guide
- `docs/MASTER_DEVELOPMENT_JOURNAL.md` - Full development history
- `docs/API_DEVELOPMENT_JOURNAL.md` - API development progress

## âš¡ Auto-Scraping System

The background scheduler automatically:
- **Scrapes MOSDAC website** every 48 hours (configurable)
- **Ingests scraped data** into the vector database
- **Monitors system health** every 5 minutes
- **Collects performance metrics** regularly

## ğŸš€ Production Deployment

### Using Gunicorn + Uvicorn
```bash
pip install gunicorn uvloop httptools
gunicorn src.api.main:app --workers 4 --worker-class uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000
```

### Environment Variables
- `LOG_LEVEL`: Logging level (INFO, DEBUG, WARNING, ERROR)
- `LLM_PROVIDER`: LLM service provider
- `LLM_API_KEY`: API key for LLM service
- `DATABASE_URL`: Vector database connection string

## ğŸš¨ Troubleshooting

### Common Issues
1. **API won't start**: Check dependencies and Python version
2. **Scraping fails**: Verify network connectivity and website availability
3. **LLM not responding**: Check LLM configuration and API keys
4. **Vector DB errors**: Verify ChromaDB installation and permissions

### Debug Mode
```bash
export LOG_LEVEL=DEBUG
python -m src.api.main
```

## ğŸ“ˆ Performance

- **Scraping**: 443 URLs in ~15 minutes
- **Ingestion**: 708 chunks in ~110 seconds
- **Retrieval**: Sub-second response times
- **Chat**: Real-time natural language responses
- **API**: Handles 100+ requests per minute

## ğŸ›¡ï¸ Quality Assurance

- **Source Citations**: Every response includes source references
- **Quality Scoring**: Automated content quality assessment
- **Session Management**: Context retention across conversations
- **Error Handling**: Graceful failure recovery
- **Rate Limiting**: API protection against abuse

## ğŸ”„ Available Operations

1. **Scrape Data Only** - Extract all MOSDAC content
2. **Ingest Data Only** - Process scraped data into vector DB
3. **Scrape + Ingest** - Complete workflow
4. **Chat with Bot** - Interactive Q&A
5. **Check Data Status** - View system status
6. **Remove All Data** - Clean up data
7. **Re-scrape + Re-ingest** - Full refresh
8. **API Access** - RESTful interface for all operations

## ğŸ¤– LLM Configuration

The bot supports two LLM modes:

### API Mode (Default)
- Uses Gemini API
- Faster, no local setup required
- Requires `GEMINI_API_KEY`

### Ollama Mode (Offline)
- Uses local Ollama installation
- Private, offline operation
- Requires Ollama server running

---

**Status**: Fully Functional âœ…  
**API Status**: Production-Ready with Auto-Scraping âœ…  
**Last Updated**: September 14, 2025  
**Version**: 2.0
